---
# Python 3 support
pip_virtualenv_command: /usr/bin/python3 -m virtualenv # usegalaxy_eu.certbot, usegalaxy_eu.tiaas2, galaxyproject.galaxy
certbot_virtualenv_package_name: python3-virtualenv    # usegalaxy_eu.certbot
pip_package: python3-pip                               # geerlingguy.pip

# PostgreSQL
postgresql_objects_users:
  - name: galaxy
postgresql_objects_databases:
  - name: galaxy
    owner: galaxy
# PostgreSQL Backups
postgresql_backup_dir: /data/backups
postgresql_backup_local_dir: "~postgres/backups"

# Galaxy
galaxy_create_user: true
galaxy_separate_privileges: true
galaxy_manage_paths: true
galaxy_layout: root-dir
galaxy_root: /srv/galaxy
galaxy_user: {name: galaxy, shell: /bin/bash}
galaxy_commit_id: release_20.09
galaxy_force_checkout: true
miniconda_prefix: "{{ galaxy_tool_dependency_dir }}/_conda"
miniconda_version: 4.7.12
miniconda_manage_dependencies: false

galaxy_config:
  galaxy:
    dependency_resolvers_config_file: "{{ galaxy_config_dir }}/dependency_resolvers_conf.xml"
    containers_resolvers_config_file: "{{ galaxy_config_dir }}/container_resolvers_conf.xml"
    brand: "ðŸ§¬ðŸ”¬ðŸš€"
    admin_users: tomas.klingstrom@slu.se
    database_connection: "postgresql:///galaxy?host=/var/run/postgresql"
    file_path: /data
    check_migrate_tools: false
    tool_data_path: "{{ galaxy_mutable_data_dir }}/tool-data"
    job_config_file: "{{ galaxy_config_dir }}/job_conf.xml"
    tool_data_table_config_path: "/cvmfs/data.galaxyproject.org/byhand/location/tool_data_table_conf.xml,/cvmfs/data.galaxyproject.org/managed/location/tool_data_table_conf.xml"
    library_import_dir: /libraries/admin
    user_library_import_dir: /libraries/user

  uwsgi:
    socket: 127.0.0.1:8080
    buffer-size: 16384
    processes: 1
    threads: 4
    offload-threads: 2
    static-map:
      - /static={{ galaxy_server_dir }}/static
      - /favicon.ico={{ galaxy_server_dir }}/static/favicon.ico
    static-safe: client/galaxy/images
    master: true
    virtualenv: "{{ galaxy_venv_dir }}"
    pythonpath: "{{ galaxy_server_dir }}/lib"
    module: galaxy.webapps.galaxy.buildapp:uwsgi_app()
    thunder-lock: true
    die-on-term: true
    hook-master-start:
      - unix_signal:2 gracefully_kill_them_all
      - unix_signal:15 gracefully_kill_them_all
    py-call-osafterfork: true
    enable-threads: true
    mule:
      - lib/galaxy/main.py
      - lib/galaxy/main.py
    farm: job-handlers:1,2

galaxy_config_templates:
  - src: templates/galaxy/config/job_conf.xml.j2
    dest: "{{ galaxy_config.galaxy.job_config_file }}"
  - src: templates/galaxy/config/container_resolvers_conf.xml.j2
    dest: "{{ galaxy_config.galaxy.containers_resolvers_config_file }}"

galaxy_config_files:
 - src: files/galaxy/config/dependency_resolvers_conf.xml
   dest: "{{ galaxy_config.galaxy.dependency_resolvers_config_file }}"

galaxy_systemd_mode: mule
galaxy_zergpool_listen_addr: 127.0.0.1:8080
galaxy_restart_handler_name: "Restart Galaxy"
galaxy_systemd_zergling_env: DRMAA_LIBRARY_PATH="/usr/lib/slurm-drmaa/lib/libdrmaa.so.1"


# Certbot
certbot_auto_renew_hour: "{{ 23 |random(seed=inventory_hostname)  }}"
certbot_auto_renew_minute: "{{ 59 |random(seed=inventory_hostname)  }}"
certbot_auth_method: --webroot
certbot_install_method: virtualenv
certbot_auto_renew: yes
certbot_auto_renew_user: root
certbot_environment: staging
certbot_well_known_root: /srv/nginx/_well-known_root
certbot_share_key_users:
  - nginx
certbot_post_renewal: |
    systemctl restart nginx || true
certbot_domains:
 - "{{ inventory_hostname }}"
certbot_agree_tos: --agree-tos

# NGINX
nginx_selinux_allow_local_connections: true
nginx_servers:
nginx_enable_default_server: false
nginx_conf_http:
  client_max_body_size: 1g

# Golang
golang_gopath: '/opt/workspace-go'
# Singularity target version
singularity_version: "3.7.0"
singularity_go_path: "{{ golang_install_dir }}"

# slurm
slurm_roles: ['controller', 'exec'] # Which roles should the machine play? exec are execution hosts.
slurm_nodes:
- name: localhost # Name of our host
  CPUs: 2         # Here you would need to figure out how many cores your machine has. For this training we will use 2 but in real life, look at `htop` or similar.
slurm_config:
  SlurmdParameters: config_overrides   # Ignore errors if the host actually has cores != 2
  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory  # Allocate individual cores/memory instead of entire node
